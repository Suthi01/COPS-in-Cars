{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mAdXLkLZPqw"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import mediapipe as mp\n",
        "\n",
        "# Load YOLOv5 model for detecting objects (humans)\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)\n",
        "\n",
        "# Initialize MediaPipe Pose for pose estimation\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# Function to detect objects (humans) in the video frame\n",
        "def detect_objects(frame):\n",
        "    results = model(frame)\n",
        "    detections = []\n",
        "    if len(results.xyxy[0]) > 0:\n",
        "        for *xyxy, conf, cls in results.xyxy[0]:\n",
        "            label = model.names[int(cls)]\n",
        "            if label == 'person' and conf > 0.5:  # Confidence threshold to detect humans\n",
        "                x1, y1, x2, y2 = map(int, xyxy)\n",
        "                detections.append({\"label\": label, \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2, \"confidence\": conf})\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "    return frame, detections\n",
        "\n",
        "# Function to track the motion of humans in the video and classify as alive or dead\n",
        "def track_human_activity(all_detections):\n",
        "    active_people = 0\n",
        "    inactive_people = 0\n",
        "    unique_persons = set()  # A set to track unique persons across frames\n",
        "    person_last_positions = {}  # Stores the last known position of each person\n",
        "    movement_threshold = 50  # Minimum movement distance to be considered \"alive\"\n",
        "    inactivity_threshold = 10  # Number of consecutive frames with no movement to be considered \"dead\"\n",
        "\n",
        "    # Analyze the movement of each person\n",
        "    for frame_data in all_detections:\n",
        "        detections = frame_data[\"detections\"]\n",
        "\n",
        "        for detection in detections:\n",
        "            label = detection[\"label\"]\n",
        "            if label == \"person\":\n",
        "                # Use the bounding box (x1, y1, x2, y2) as a unique identifier for a person\n",
        "                person_id = (detection[\"x1\"], detection[\"y1\"], detection[\"x2\"], detection[\"y2\"])\n",
        "\n",
        "                # Track unique persons across frames using the bounding box\n",
        "                if person_id not in unique_persons:\n",
        "                    unique_persons.add(person_id)  # Add person to the unique set\n",
        "\n",
        "                track_id = len(unique_persons)  # Track ID is the number of unique persons\n",
        "\n",
        "                x1, y1, x2, y2 = detection[\"x1\"], detection[\"y1\"], detection[\"x2\"], detection[\"y2\"]\n",
        "                center_x = (x1 + x2) // 2\n",
        "                center_y = (y1 + y2) // 2\n",
        "\n",
        "                # Calculate movement: Compare the person's center position with the previous frame's position\n",
        "                if track_id in person_last_positions:\n",
        "                    prev_x, prev_y = person_last_positions[track_id]\n",
        "                    distance_moved = ((center_x - prev_x) ** 2 + (center_y - prev_y) ** 2) ** 0.5\n",
        "\n",
        "                    # If the person moved more than the threshold, consider them as \"active\"\n",
        "                    if distance_moved > movement_threshold:\n",
        "                        active_people += 1\n",
        "                    else:\n",
        "                        inactive_people += 1\n",
        "\n",
        "                # Update the last known position of the person\n",
        "                person_last_positions[track_id] = (center_x, center_y)\n",
        "\n",
        "    return active_people, inactive_people, len(unique_persons)\n",
        "\n",
        "# Function to generate a conclusion based on the activity (alive or dead)\n",
        "def generate_activity_report(all_detections):\n",
        "    active_people, inactive_people, total_people = track_human_activity(all_detections)\n",
        "\n",
        "    conclusion = \"\\n--- Conclusion ---\\n\"\n",
        "    conclusion += f\"Total number of unique people detected: {total_people}\\n\"\n",
        "    conclusion += f\"Active people (alive): {active_people}\\n\"\n",
        "    conclusion += f\"Inactive people (dead): {inactive_people}\\n\"\n",
        "\n",
        "    if active_people > inactive_people:\n",
        "        conclusion += \"Most of the people in the video appear to be alive, with some inactive individuals.\"\n",
        "    else:\n",
        "        conclusion += \"The video shows a higher number of inactive individuals, suggesting some may be dead or unconscious.\"\n",
        "\n",
        "    return conclusion\n",
        "\n",
        "# Function to process the video and generate the human actions story with alive/dead classification\n",
        "def generate_video_story_with_status(video_file):\n",
        "    if not os.path.exists(video_file):\n",
        "        print(f\"Error: Video file '{video_file}' not found.\")\n",
        "        return\n",
        "\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    all_detections = []\n",
        "    frame_count = 0\n",
        "    story_text = \"\"\n",
        "\n",
        "    if cap.isOpened():\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # Detect objects and human (person) in the video\n",
        "                frame, detections = detect_objects(frame)\n",
        "                all_detections.append({\"frame\": frame_count, \"detections\": detections})\n",
        "\n",
        "                # Display the frame (optional)\n",
        "                cv2.imshow('Frame', frame)\n",
        "                cv2.waitKey(1)\n",
        "\n",
        "                # Stop on 'q' key press\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "                frame_count += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "        # Generate conclusion for the activity report (alive vs. dead)\n",
        "        conclusion = generate_activity_report(all_detections)\n",
        "        story_text += conclusion\n",
        "\n",
        "        # Save the generated story to a text file\n",
        "        story_file = r\"C:\\Users\\Administrator\\Downloads\\New Text Document (2).txt\"\n",
        "        try:\n",
        "            with open(story_file, \"w\") as f:\n",
        "                f.write(story_text)\n",
        "            print(f\"Story saved to {story_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving story to file: {e}\")\n",
        "\n",
        "        return story_text\n",
        "    else:\n",
        "        print(\"Could not open video file.\")\n",
        "        return None\n",
        "\n",
        "# Main function to execute the process\n",
        "video_file = r\"C:\\Users\\Administrator\\Downloads\\WhatsApp Video 2025-02-20 at 13.45.59.mp4\"  # Replace with your video file path\n",
        "generate_video_story_with_status(video_file)\n"
      ]
    }
  ]
}